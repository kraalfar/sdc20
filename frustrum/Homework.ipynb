{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "ML разработчикам необходимо не только уметь обучать нейронные сети и генерировать новые идеи, но еще и уметь встраивать наработки в pipeline. В этом домашнем задании нам предстоит сделать из frustum детектора production ready (ну почти) решение, которое может работать на сырых данных.\n",
    "\n",
    "К сожалению, frustum-pointnet работает независимо для каждой 2D детекции. В этом домашнем задании вам предстоит написать обертку над frustum-pointnet, которая будет работать над целыми облаками. Вам также нужно будет воспользоваться 2D детектором, чтобы находить коробки на изображении.\n",
    "\n",
    "Во второй части задания вам нужно будет написать оценку качества работы вашего алгоритма, которая становится чуть сложнее, когда на сцене могут находится много объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1918,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from ssd import SSD\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10., 7.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_util import NUM_HEADING_BIN, NUM_SIZE_CLUSTER\n",
    "import provider\n",
    "from train_util import get_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1\n",
    "\n",
    "Ниже написан класс, который вам нужно реализовать. Чтобы воспользоваться предобученной сетью, позаимствуйте код из\n",
    "https://github.com/charlesq34/frustum-pointnets/blob/master/train/test.py\n",
    "\n",
    "Предобученные модели лежат здесь: https://shapenet.cs.stanford.edu/media/frustum_pointnets_snapshots.zip\n",
    "\n",
    "В частности, вам нужно модифицировать функцию `get_session_and_ops` - функция должна уметь работать без глобальных флагов. После этого посмотрите, как эта функция используется.\n",
    "Выход сети преобразуется в понятный формат в функции `write_detection_results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Некоторые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car2cam(path):\n",
    "    P2 = None\n",
    "    R0_rect = None\n",
    "    Tr_velo_to_cam = None\n",
    "    with open(path) as f:\n",
    "        raw = f.read()\n",
    "        raw = raw[:-2].split('\\n')\n",
    "        P2_vals = np.array(list(map(float, raw[2].split()[1:])))\n",
    "        P2 = P2_vals.reshape(3, 4)\n",
    "        \n",
    "        R0_vals = np.array(list(map(float, raw[4].split()[1:])))\n",
    "        R0_rect = np.zeros((4, 4))\n",
    "        R0_rect[3, 3] = 1\n",
    "        R0_rect[:3, :3] = R0_vals.reshape(3, 3)\n",
    "        \n",
    "        Tr_vals = np.array(list(map(float, raw[5].split()[1:])))\n",
    "        Tr_velo_to_cam = np.zeros((4, 4))\n",
    "        Tr_velo_to_cam[3, 3] = 1\n",
    "        Tr_velo_to_cam[:3, :4] = Tr_vals.reshape(3, 4)\n",
    "        \n",
    "    return P2, R0_rect, Tr_velo_to_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_in_box(box, points):\n",
    "    y_test = (box[0] <= points[:,1]) * (box[2] >= points[:,1])\n",
    "    x_test = (box[1] <= points[:,0]) * (box[3] >= points[:,0])\n",
    "    return x_test * y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1819,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_and_ops(model, batch_size, num_point):\n",
    "    \"\"\" Define model graph, load model parameters,\n",
    "    create session and return session handle and tensors\n",
    "    \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:0'):\n",
    "            pointclouds_pl, one_hot_vec_pl, labels_pl, centers_pl, \\\n",
    "            heading_class_label_pl, heading_residual_label_pl, \\\n",
    "            size_class_label_pl, size_residual_label_pl = \\\n",
    "                model.placeholder_inputs(batch_size, num_point)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            end_points = model.get_model(pointclouds_pl, one_hot_vec_pl,\n",
    "                                         is_training_pl)\n",
    "            loss = model.get_loss(labels_pl, centers_pl,\n",
    "                                  heading_class_label_pl, heading_residual_label_pl,\n",
    "                                  size_class_label_pl, size_residual_label_pl, end_points)\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, 'train/log_v1/model.ckpt')\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'one_hot_vec_pl': one_hot_vec_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'centers_pl': centers_pl,\n",
    "               'heading_class_label_pl': heading_class_label_pl,\n",
    "               'heading_residual_label_pl': heading_residual_label_pl,\n",
    "               'size_class_label_pl': size_class_label_pl,\n",
    "               'size_residual_label_pl': size_residual_label_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'logits': end_points['mask_logits'],\n",
    "               'center': end_points['center'],\n",
    "               'end_points': end_points,\n",
    "               'loss': loss}\n",
    "        return sess, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sess, ops, pc, one_hot_vec, batch_size):\n",
    "    \"\"\" Run inference for frustum pointnets in batch mode \"\"\"\n",
    "    assert pc.shape[0] % batch_size == 0\n",
    "    num_batches = int(pc.shape[0] / batch_size)\n",
    "    logits = np.zeros((pc.shape[0], pc.shape[1], 2))\n",
    "    centers = np.zeros((pc.shape[0], 3))\n",
    "    heading_logits = np.zeros((pc.shape[0], 12))\n",
    "    heading_residuals = np.zeros((pc.shape[0], 12))\n",
    "    size_logits = np.zeros((pc.shape[0], 8))\n",
    "    size_residuals = np.zeros((pc.shape[0], 8, 3))\n",
    "    scores = np.zeros((pc.shape[0],))  # 3D box score\n",
    "\n",
    "    ep = ops['end_points']\n",
    "    for i in range(num_batches):\n",
    "        feed_dict = { \\\n",
    "            ops['pointclouds_pl']: pc[i * batch_size:(i + 1) * batch_size, ...],\n",
    "            ops['one_hot_vec_pl']: one_hot_vec[i * batch_size:(i + 1) * batch_size, :],\n",
    "            ops['is_training_pl']: False}\n",
    "\n",
    "        batch_logits, batch_centers, \\\n",
    "        batch_heading_scores, batch_heading_residuals, \\\n",
    "        batch_size_scores, batch_size_residuals = \\\n",
    "            sess.run([ops['logits'], ops['center'],\n",
    "                      ep['heading_scores'], ep['heading_residuals'],\n",
    "                      ep['size_scores'], ep['size_residuals']],\n",
    "                     feed_dict=feed_dict)\n",
    "\n",
    "        logits[i * batch_size:(i + 1) * batch_size, ...] = batch_logits\n",
    "        centers[i * batch_size:(i + 1) * batch_size, ...] = batch_centers\n",
    "        heading_logits[i * batch_size:(i + 1) * batch_size, ...] = batch_heading_scores\n",
    "        heading_residuals[i * batch_size:(i + 1) * batch_size, ...] = batch_heading_residuals\n",
    "        size_logits[i * batch_size:(i + 1) * batch_size, ...] = batch_size_scores\n",
    "        size_residuals[i * batch_size:(i + 1) * batch_size, ...] = batch_size_residuals\n",
    "\n",
    "    heading_cls = np.argmax(heading_logits, 1)  # B\n",
    "    size_cls = np.argmax(size_logits, 1)  # B\n",
    "    heading_res = np.array([heading_residuals[i, heading_cls[i]] \\\n",
    "                            for i in range(pc.shape[0])])\n",
    "    size_res = np.vstack([size_residuals[i, size_cls[i], :] \\\n",
    "                          for i in range(pc.shape[0])])\n",
    "\n",
    "    return np.argmax(logits, 2), centers, heading_cls, heading_res, \\\n",
    "           size_cls, size_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(box, P):\n",
    "    box2d_center = np.array([(box[1]+box[3])/2.0, (box[0]+box[1])/2.0])\n",
    "    uv_depth = np.zeros((1,3))\n",
    "    uv_depth[0,0:2] = box2d_center\n",
    "    uv_depth[0,2] = 20 # some random depth\n",
    "    \n",
    "    \n",
    "    c_u = P[0,2]\n",
    "    c_v = P[1,2]\n",
    "    f_u = P[0,0]\n",
    "    f_v = P[1,1]\n",
    "    b_x = P[0,3]/(-f_u) # relative \n",
    "    b_y = P[1,3]/(-f_v)\n",
    "   \n",
    "    \n",
    "    n = uv_depth.shape[0]\n",
    "    x = ((uv_depth[:,0]-c_u)*uv_depth[:,2])/f_u + b_x\n",
    "    y = ((uv_depth[:,1]-c_v)*uv_depth[:,2])/f_v + b_y\n",
    "    box2d_center_rect = np.zeros((n,3))\n",
    "    box2d_center_rect[:,0] = x\n",
    "    box2d_center_rect[:,1] = y\n",
    "    box2d_center_rect[:,2] = uv_depth[:,2]\n",
    "    frustum_angle = -1 * np.arctan2(box2d_center_rect[0,2], box2d_center_rect[0,0])\n",
    "    return frustum_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1761,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detection = namedtuple('Detection', ['xyz', 'angle', 'lwh', 'confidence'])\n",
    "Scene = namedtuple('Scene', ['detections'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PipelineDetector(object):\n",
    "    def __init__(self, frustum_pointnet, frustum_batch_size, ssd_detector, ssd_threshold):\n",
    "        self.frustrum_pointnet = frustum_pointnet\n",
    "        self.frustrum_bs = frustum_batch_size\n",
    "        self.ssd_detector = ssd_detector\n",
    "        self.ssd_thr = ssd_threshold\n",
    "    \n",
    "    def predict(self, xyz, image, car_to_cam, intrinsics, sess=None):\n",
    "        P2, R0_rect, Tr_velo_to_cam = car_to_cam\n",
    "        xyz2image = (P2 @ R0_rect @ Tr_velo_to_cam).T\n",
    "        xyz2xyz = (R0_rect @ Tr_velo_to_cam).T\n",
    "        \n",
    "        \n",
    "        # project xyz into image\n",
    "        xyzh = xyzi.copy()\n",
    "        xyzh[:, 3] = 1\n",
    "        \n",
    "        proj = xyzh @ xyz2image\n",
    "        proj = proj[:, :2] / proj[:, 2:]\n",
    "        \n",
    "        # transform xyz (same way as in prepare data)\n",
    "        xyzf = xyzh.copy()\n",
    "        xyzf = xyzf @ xyz2xyz\n",
    "        xyzf[:, 3] = xyzi[:, 3]\n",
    "        \n",
    "        # mask for points in image\n",
    "        in_im = points_in_box([0, 0, image.shape[0], image.shape[1]], proj)\n",
    "        \n",
    "        #get sess and ops\n",
    "        if sess is None:\n",
    "            sess, ops = get_session_and_ops(self.frustrum_pointnet, batch_size=1, num_point=1024)\n",
    "        else:\n",
    "            sess, ops = sess\n",
    "        \n",
    "        # TODO: run 2D detector on the image\n",
    "        ssd_res = self.ssd_detector.predict(image)\n",
    "        boxes = ssd_res[\"detection_boxes\"][:ssd_res['num_detections']]\n",
    "        boxes[:, [0, 2]] = boxes[:, [0, 2]] * image.shape[0]\n",
    "        boxes[:, [1, 3]] = boxes[:, [1, 3]] * image.shape[1]\n",
    "        scores = ssd_res[\"detection_scores\"][:ssd_res['num_detections']]\n",
    "        classes = ssd_res[\"detection_classes\"][:ssd_res['num_detections']]\n",
    "        \n",
    "        # TODO: extract bounding boxes with vehicle classes and filter them by ssd_threshold\n",
    "        boxes = boxes[(classes == 1) * (scores > self.ssd_thr)]\n",
    "        scores = scores[(classes == 1) * (scores > self.ssd_thr)]\n",
    "        # TODO: process lidar point cloud and construct frustum examples\n",
    "       \n",
    "        frustrums = []\n",
    "        angles = []\n",
    "        for box in boxes:\n",
    "            ind = points_in_box(box, proj)\n",
    "            pc = xyzf[ind * in_im]\n",
    "            \n",
    "            frustrums.append(pc[np.random.choice(pc.shape[0], 1024)].reshape(1, 1024, 4))\n",
    "            angles.append(get_angle(box, P2))\n",
    "#         frustrums = np.vstack(frustrums)   \n",
    "        \n",
    "        # TODO: run frustum inference (use batch to accelerate inference per frame)\n",
    "        results = []\n",
    "        one_hot = np.zeros((1, 3))\n",
    "        one_hot[:,0] = 1\n",
    "        for i in range(len(frustrums)):\n",
    "            batch_output, batch_center_pred, \\\n",
    "            batch_hclass_pred, batch_hres_pred, \\\n",
    "            batch_sclass_pred, batch_sres_pred = inference(sess, ops, \n",
    "                                                           frustrums[i], \n",
    "                                                           one_hot, \n",
    "                                                           batch_size=1)\n",
    "            \n",
    "            \n",
    "            h,w,l,tx,ty,tz,ry = provider.from_prediction_to_label_format(batch_center_pred[0],\n",
    "            batch_hclass_pred[0], batch_hres_pred[0],\n",
    "            batch_sclass_pred[0], batch_sres_pred[0], angles[i])\n",
    "            detec = Detection(xyz=np.array([tx, ty, tz]), \n",
    "                              angle=ry, \n",
    "                              lwh=np.array([l, w, h]), \n",
    "                              confidence=scores[i])\n",
    "            results.append(detec)\n",
    "        # TODO: construct Scene namedtuple and return it\n",
    "        scene = Scene(results)\n",
    "        return scene\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = '000009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name 3d mask loss is illegal; using 3d_mask_loss instead.\n",
      "INFO:tensorflow:Summary name center loss is illegal; using center_loss instead.\n",
      "INFO:tensorflow:Summary name stage1 center loss is illegal; using stage1_center_loss instead.\n",
      "INFO:tensorflow:Summary name heading class loss is illegal; using heading_class_loss instead.\n",
      "INFO:tensorflow:Summary name heading residual normalized loss is illegal; using heading_residual_normalized_loss instead.\n",
      "INFO:tensorflow:Summary name size class loss is illegal; using size_class_loss instead.\n",
      "INFO:tensorflow:Summary name size residual normalized loss is illegal; using size_residual_normalized_loss instead.\n",
      "INFO:tensorflow:Summary name corners loss is illegal; using corners_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from train/log_v1/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = importlib.import_module('frustum_pointnets_v1')\n",
    "detector = SSD('tmp')\n",
    "im =  np.array(Image.open(f'training/image_2/{img}.png'))\n",
    "car_to_cam = car2cam(f'training/calib/{img}.txt')\n",
    "xyzi = np.fromfile(f'training/velodyne/{img}.bin', dtype=np.float32).reshape(-1, 4)\n",
    "sess = get_session_and_ops(model, batch_size=1, num_point=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PipelineDetector(model, 1, detector, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipe.predict(xyzi, im, car_to_cam, None, sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detection(xyz=array([-23.91640924,   1.71225619,   1.29680484]), angle=-3.0798698553950263, lwh=array([3.52497799, 1.65690606, 1.54393064]), confidence=1.0)"
      ]
     },
     "execution_count": 1835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detection(xyz=array([-11.08093195,   1.45777028,  -0.51371127]), angle=0.9016395076830324, lwh=array([3.64997223, 1.59477014, 1.50140129]), confidence=1.0)"
      ]
     },
     "execution_count": 1836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.detections[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detection(xyz=array([-66.6559221 ,   1.97147547,   0.36495579]), angle=0.49187220178757607, lwh=array([3.89526882, 1.59805638, 1.50842306]), confidence=0.9999999)"
      ]
     },
     "execution_count": 1837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.detections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car 0.00 0 -1.50 601.96 177.01 659.15 229.51 1.61 1.66 3.20 0.70 1.76 23.88 -1.48\n",
      "Car 0.00 2 1.75 600.14 177.09 624.65 193.31 1.44 1.61 3.66 0.24 1.84 66.37 1.76\n",
      "Car 0.00 0 1.78 574.98 178.64 598.45 194.01 1.41 1.53 3.37 -2.19 1.96 68.25 1.75\n",
      "DontCare -1 -1 -10 710.60 167.73 736.68 182.35 -1 -1 -1 -1000 -1000 -1000 -10\n",
      "DontCare -1 -1 -10 758.52 156.27 782.52 179.23 -1 -1 -1 -1000 -1000 -1000 -10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'training/label_2/{img}.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Часть 2\n",
    "\n",
    "Для оценки качества работы 3D детекторов обычно используется average precision. Как измерить precision и recall детектора?\n",
    "\n",
    "У каждой коробки детектора есть confidence. После того, как мы зафиксировали порог, у нас остается часть детекций.\n",
    "Давайте теперь посмотрим на сцену сверху: bird's eye view. Забудем про координату z.\n",
    "\n",
    "Далее мы можем посчитать IoU между всеми коробками ground truth и нашими детекциями.Давайте решим, что если IoU больше 0.7, то мы будем считать, что мы увидели gt коробку - относим эту детекцию к TP. Если gt не нашла пару - False Negative. Если детекция не нашла пару - False Positive.\n",
    "\n",
    "Ваша задача написать код подсчета метрики average precision построенного детектора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Kitti Dataset\n",
    "[Kitti dev-kit](https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip) : \n",
    "там можно найти описание данных и как преобразовывать данные между системами координат.\n",
    "\n",
    "Homework KITTI dataset :https://www.icloud.com/iclouddrive/0bxlXWgCRTVvsWXmd-kcEqKqA#kitti_hw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1846,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_boxes(scene):\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    for detec in scene.detections:\n",
    "        lx, ly = detec.xyz[0], detec.xyz[1]\n",
    "        rx, ry = lx + detec.lwh[0], ly + detec.lwh[1]\n",
    "        boxes.append(np.array([lx, ly, rx, ry]))\n",
    "        scores.append(detec.confidence)\n",
    "    return np.vstack((boxes)), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1910,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_boxes(img):\n",
    "    boxes = []\n",
    "    with open(f'training/label_2/{img}.txt') as f:\n",
    "        for l in f.readlines():\n",
    "            data = l.split()\n",
    "            if data[0] == \"Car\":\n",
    "                box = list(map(float, [data[4], data[5], data[6], data[7]]))\n",
    "                boxes.append(box)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "    return np.vstack((boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(bboxes1, bboxes2):\n",
    "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1839,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = '000009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name 3d mask loss is illegal; using 3d_mask_loss instead.\n",
      "INFO:tensorflow:Summary name center loss is illegal; using center_loss instead.\n",
      "INFO:tensorflow:Summary name stage1 center loss is illegal; using stage1_center_loss instead.\n",
      "INFO:tensorflow:Summary name heading class loss is illegal; using heading_class_loss instead.\n",
      "INFO:tensorflow:Summary name heading residual normalized loss is illegal; using heading_residual_normalized_loss instead.\n",
      "INFO:tensorflow:Summary name size class loss is illegal; using size_class_loss instead.\n",
      "INFO:tensorflow:Summary name size residual normalized loss is illegal; using size_residual_normalized_loss instead.\n",
      "INFO:tensorflow:Summary name corners loss is illegal; using corners_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from train/log_v1/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = importlib.import_module('frustum_pointnets_v1')\n",
    "detector = SSD('tmp')\n",
    "im =  np.array(Image.open(f'training/image_2/{img}.png'))\n",
    "car_to_cam = car2cam(f'training/calib/{img}.txt')\n",
    "xyzi = np.fromfile(f'training/velodyne/{img}.bin', dtype=np.float32).reshape(-1, 4)\n",
    "sess = get_session_and_ops(model, batch_size=1, num_point=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1893,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = importlib.import_module('frustum_pointnets_v1')\n",
    "sess = get_session_and_ops(model, batch_size=1, num_point=1024)\n",
    "pipe = PipelineDetector(model, 1, detector, 0)\n",
    "img_dir = os.listdir('training/image_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1912,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = None\n",
    "scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1914,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in img_dir:\n",
    "    img = img_name.split('.')[0]\n",
    "    im =  np.array(Image.open(f'training/image_2/{img}.png'))\n",
    "    car_to_cam = car2cam(f'training/calib/{img}.txt')\n",
    "    xyzi = np.fromfile(f'training/velodyne/{img}.bin', dtype=np.float32).reshape(-1, 4)\n",
    "    \n",
    "    \n",
    "    res = pipe.predict(xyzi, im, car_to_cam, None, sess=sess)\n",
    "    if len(res.detections) == 0:\n",
    "        continue\n",
    "    preds, cur_scores = get_pred_boxes(res)\n",
    "    gts = get_gt_boxes(img)\n",
    "    if gts is None:\n",
    "        continue\n",
    "    ious = iou(preds, gts)\n",
    "    inds = np.argmax(ious, axis=1)\n",
    "    best_ious = ious[np.arange(preds.shape[0]), inds]\n",
    "    cur_corr = (best_ious > 0.7).astype(int)\n",
    "    \n",
    "    if corr is None:\n",
    "        corr = cur_corr\n",
    "        scores = cur_scores\n",
    "    else:\n",
    "        corr = np.hstack((corr, cur_corr))\n",
    "        scores = np.hstack((scores, cur_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1922,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(corr, scores):\n",
    "    ind = np.argsort(scores)\n",
    "    corr = corr[ind]\n",
    "    TP = np.cumsum(corr)\n",
    "    TP_plus_FP = np.arange(1, len(corr) + 1)\n",
    "    acc = TP / TP_plus_FP\n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision(corr, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1924,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1925,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://wompampsupport.azureedge.net/fetchimage?siteId=7575&v=2&jpgQuality=100&width=700&url=https%3A%2F%2Fi.kym-cdn.com%2Fentries%2Ficons%2Ffacebook%2F000%2F028%2F021%2Fwork.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Im(url='https://wompampsupport.azureedge.net/fetchimage?siteId=7575&v=2&jpgQuality=100&width=700&url=https%3A%2F%2Fi.kym-cdn.com%2Fentries%2Ficons%2Ffacebook%2F000%2F028%2F021%2Fwork.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
